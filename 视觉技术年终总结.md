# 视觉技术年终总结

## 1. 摘要 

对笔者24赛季在RM视觉领域学到的知识做一个浅显的总结，主要从接触自瞄，前期学习，到上手实践等方面阐开叙述:smile:

部分内容参照湖南大学跃鹿战队曾庆铖学长写的《了解CV和RoboMaster视觉组——你的最后一本计算机视觉入门手册》，==强烈建议所有刚刚入门视觉组的小白去看这本书==。（RM视觉救命群的群文档中有）

## 2. RM自瞄

>救命呀
>
>> > 2.7.3

***

### 2.1 自瞄与视觉组

计算机视觉(computer vision)无疑是当今AI界最火热的研究领域之一，自然而然，在RM的赛场上，视觉的软硬件开发也占有一席之地，视觉组便和其他技术组一样应运而生了。

视觉组负责的机器人模块主要是传感器和数据处理，即通过对相机、激光雷达等传感器采集到的信息进行处理从而让机器人在一定程度上具有”视觉”或“知觉”功能。

### 2.2 自瞄在赛场中的体现

- 在比赛中，视觉组能够让机器人自动识别地方装甲板，实现“自瞄外挂”；也能让操作手轻松地击打能量机关，使得全队获得增益。无人机在高空中击打地面单位，有着自瞄将会是如虎添翼。同时工程机器人面临复杂的对矿角度有视觉识别自动兑矿也将易如反掌。像哨兵这种自动机器人有着上位机优秀的感知识别也会大大增加战斗力。

- 自瞄的整体思路即先从相机获取到当前画面状态，在上位机通过特定图像处理（可分为传统自瞄和深度学习）对所需要的特定物体框选出来（如装甲板或能量机关）,然后对这个点位进行姿态解算，运动预测，击打补偿等，从而获取精准的射击点，最后将此射击点通过特定串口通信发送给下位机从而使得机器人云台的转动及瞄准。

- 其中的每一部分都是不可或缺的，将每个环节做好最后才会做出好的自瞄，其中本赛季除了将识别的神经网络模型进行优化外，更多的研发重点在上位机的运动预测和击打补偿的开发。


## 3. 学习路线

### 3.1 知识储备

- C++；opencv；
- ( cmake；python；pytorch；yolov5；Linux多线程pthread库；ROS等)；
- 装甲板识别（指学习完opencv后使用基本的阈值图像处理对装甲板进行识别框选）；

- 测距算法（对框选出的装甲板进行初步的姿态解算，如pnp测距和三角形测距，可得到目标当前位置xyz，目标在相机坐标系下的角度yaw,pitch,及目标距离distance）


到此算对视觉组的一些基础知识有了掌握，开始开启另一个系统--Linux :dart:

### 3.2 视觉组接触的软件

#### 3.2.1 Ubuntu

Ubuntu是一个Debian系分支的第一大系统，是当前用户量最大的linux发行版，Linux下开发C++程序相比Windows有无与伦比的优势，可以方便的配置各种第三方库和依赖。Linux对于深度学习的支持比Widnows更加友好，经常有sh脚本能够一键配置开发环境。而且Linux对一些设备驱动的支持也更完善，我们可以选择挂载自己需要的驱动和IO，同时对于conda环境的配置等也很是方便。

- 安装Ubuntu，有两种方法，双系统和虚拟机；
  - 双系统就是在电脑单独开一块空间进行安装Ubuntu操作系统，这也就意味内存的使用是不断增大的，同时如果在Ubuntu下不当操作会导致系统崩溃，将会获得重装系统的大礼包。当然既然安装的独立的操作系统，硬件的使用也是电脑的自带的硬件设备，同时流畅度也很可观。
  - 另一种就是Vmware虚拟机的使用，是通过vm这个软件在Windows下建立一个虚拟的Ubuntu环境学习，因此这个环境里面的硬件设备都是虚拟映射windows的（包括网络，显卡等），所以使用体验感会有相对下降，可以通过提高创建环境时的初始配置从而得到改善，小白使用虚拟机也是不错的选择，这能给你更大的试错空间，不用担心把系统搞奔溃。同时可以在Windows下边观看教学视频边学习使用Ubuntu。

- 在Ubuntu下，终端窗口和命令行的使用极其广泛普遍，因此命令行的使用需要学习（cd,ls,pwd,mv,cp,touch,rm等不多赘述）。

- 至少掌握一个无GUI的文本编辑器的基本使用，如vi，vim，nano等。这能够帮助你在系统出现问题的时候快速修改一些配置文件，或是在使用ssh连接的时候简单地编写一些程序。当然**Typora**也是不错的选择，一款不错markdown编辑器。

  
  
  本文就是使用typora编辑器编写的md文件，这款软件还具有将文章转化成pdf，word的功能，深得我心！

#### 3.2.2 IDE

- **Visual Studio** 

  这款软件将会是在windows下学习opencv并初步认识并作出装甲板识别的第一款软件，它可谓是保姆级编译器，安装完opencv库便可食用opencv这本书了，但是明显的缺点就是安装不易，内存过大，运行缓存也挺大，在Ubuntu下并不推荐使用。

- ==<u>**Visual Studio Code**</u>==

  啥系统都能用。丰富的插件生态只有你想不到没有你找不到，配置完之后使用起来非常方便，比如C++就有一个C++ extensions pack。官方文档也很详细，毕竟是微软主推的下一代编辑器。关键是好看啊！智能提示也足够智能。想要写其他的语言也能够一条龙配齐，总之，上手容易且可定制化程度极高。windows用其写，读代码也是很香。

  **这里需要特别提及的是CMakeLists 的编写。**Linux没有Visual Studio这样保姆级的IDE，并不存在一款能够自动为你生成makefile的软件。所以至少要学习qmake和cmake中的一种工具。这里推荐cmake，虽然比qmake的语法稍微复杂一些，但是cmake的功能非常强大，拥有非常优良的跨平台支持。学习cmake还能帮助你进一步了解程序的编译、链接过程。

- **Anaconda**

  - Anaconda是一个流行的Python和R语言的分发版，用于科学计算（数据科学、机器学习应用、大数据处理等）。它旨在简化包管理和部署。Anaconda分发版附带了conda——一个包和环境管理器。可以使用conda来创建不同的环境，以隔离不同项目的依赖关系，也可以安装、卸载软件包。

  - Anaconda的主要特点包括：

    - **包管理器**：Conda是Anaconda的包管理器，它允许您在任何系统上安装、管理和升级包和依赖关系。
    - **环境管理器**：Conda还可以用作环境管理器，让您可以为不同的项目创建独立的Python环境，这有助于解决不同项目之间的依赖冲突。
    - **大量预装包**：Anaconda预装了超过1,500个科学包及其依赖项，这使得科学计算和数据分析变得更加容易。
    - **跨平台**：Anaconda可用于Windows、macOS和Linux。

  
  - 创建和管理环境
  
     创建新环境：
  
    ```
    conda create -n myenv python=3.8
    ```
  
    激活环境：
  
    ```
    conda activate myenv
    ```
  
    退出环境：
  
    ```
    conda deactivate
    ```
  
    删除环境
  
    ~~~ 
    conda remove -n myenv --all
    ~~~
  
- **PyCharm**

  PyCharm是由JetBrains开发的一个强大的Python集成开发环境（IDE）。它被设计用于提高Python开发者的生产力，通过提供一系列强大的功能来简化开发流程。PyCharm支持多种编程语言，如Python、JavaScript、HTML等，但主要专注于Python开发。

  其中使用PyCharm的主要特性是

  1. **数据库工具**：内置数据库工具，支持SQL编辑、数据库连接、数据浏览和查询等。因此在安装完库之后可以支持使用了像pytorch这种深度学习框架的代码调试。
  2. **虚拟环境支持**：轻松创建、配置、使用不同的Python虚拟环境，包括Conda环境。使用conda环境的目的就是因为在深度学习中每个代码的使用软件包的版本可能不一样，从而创建不同的conda环境来是每个代码独立运行，隔离不同项目的依赖关系，互不影响，很是舒服，同时安装很多库不影响系统原环境，如果环境中混入shit不能使用的话一条命令删除即可。

- **MATLAB**

  MATLAB提供了强大的相机标定工具箱，可以帮助进行相机标定并获取所需的参数。相机标定是计算机视觉领域中的一个重要任务，用于确定相机的内部参数（如焦距、畸变等）和外部参数（如相机的位置和姿态）。

  同时可以对一些数据进行曲线的绘制，方便可视化数据。

当然还有很多方便的工具按需使用，上面仅推荐了一些常用的工具。

### 3.3 视觉组接触的硬件

#### 3.3.1 相机

相机是机器人的眼睛。和人眼的成像原理一样，相机通过镜头汇聚光束使他们聚集在一块半导体感光元件上（相当于视网膜）从而产生可供读取的数据。随后图像随着数据线传如miniPC等运算平台（视网膜刺激视神经传到神经冲动到大脑）。

- **相机驱动**；有特定**软件驱动**，通过其我们可以从相机读入画面，调整图像的尺寸，曝光，白平衡，增益，分辨率，对比度等数值。同时相机驱动还有**软件包**的安装，对软件包进行解压缩和.sh安装即可，windows下有SDK供我们使用，来调整画面，用来拍摄标定图片。

- **镜头**；其上有两个旋钮，一个是调焦距，一个是调光圈；我们都知道镜头是一块凸透镜，在他的两边各有一个焦点，焦点即所有平行进入透镜的光线的汇聚点。不同焦距的镜头其视距和视野范围不同，一般来说，视距大（看的远）的镜头，其视野范围小（可视角小）；而视距短（看的近些）的镜头，视野范围大，在比赛中，我们一般给步兵机器人配置广角镜头（适用近战，可视角广）或是6mm的镜头（中庸的选择，兼顾长短）。打击能量机关的步兵机器人或者吊射的英雄机器人会选择8mm、12mm的长焦镜头来获得更好的远距离成像效果，成像效果越准确，后面姿态解算获取的位置越准确。

- **相机标定**

  通过matlab对相机内参进行标定，获取运算平台包括焦距（fx，fy），光学中心（Cx，Cy），完全取决于相机本身，是相机的固有属性，只需要计算一次，可用矩阵表示如下：[fx, 0, Cx; 0, fy, cy; 0,0,1];
  畸变系数：畸变数学模型的5个参数 D = （k1，k2， P1， P2， k3）；

- **联合标定**

  使用ROS 对相机+IMU联合标定，得到陀螺仪相对于相机，相机相对于陀螺仪的旋转矩阵，以便之后的相机坐标系，陀螺仪坐标系，世界坐标系之间的坐标转换。（由于环境兼容的问题，我的相机工作空间在camera_ws，生成bmi088_imu_param.yaml（静态录制的rosbag）的在kalibr_ws，而联合标定在new_ws 同时最后的命令行的路径应注意是相对路径）
  
#### 3.3.2 运算平台

常见的运算平台有这几种：定制的minipc、Intel NUC、jetson系列、jetson tx2，工控主板/工控机

队里目前使用的有Intel NUC系列，Jetson Xavier nx 

#### 3.3.3 IMU+USB虚拟串口

目前队里使用的陀螺仪是大疆C板自带的IMU，IMU对于RoboMaster比赛十分重要，是电控实现“小陀螺”、控制平衡车的基础，也是视觉算法预测是否准确的根基。全称为Inertia measurement unit，惯性测量单元。一般包括加速度计和陀螺仪，气压计、磁力计等也可能被一起集成。

这里主要介绍对IMU输出数据的处理。预测算法的话主要使用的是其输出的四元数（quat），用其获取云台当前的位置。而这些数据是通过C板上的USB虚拟串口传输的，本赛季使用的这种遵循CDC协议的虚拟串口比之前使用的usb转ttl传输速度大大增加，而且传输错误几乎无。同时串口使用时最好做串口重映射(其实就是一种赋予串口永久权限），省的每次串口断了重连的时侯给权限。

### 3.4 比赛中使用到的算法

#### 3.4.1 opencv

相信这是每个每个视觉小白的必修课程，不管是使用传统自瞄的识别还是神经网络的识别，opencv库中的函数和数据类型的使用都是必要的

- **基本数据类型**

  - Mat：矩阵类型，能够保存图像。

  - Point：一个像素点，或者任何类型的“点”。

  - Scalar：一个四维点类，是许多函数的参数。

  - Size：同样是一对数据构成的组，一般表示一块区域或图像的宽高，有些时候可以和point互换。

  - Rect：rectangle，矩形类，拥有Point和Size成员，用于表示一块矩形的区域。

  - RotatedRect：同上，不过有额外的成员angle用于表示角度。

- **imgproc模块**

  - 画图

    ~~~c++
    circle() //画出一个颜色、大小、粗细可调的圆，一般用于标记角点等特殊位置 
    line() //在两点之间画出一条直线，用于框出目标或作为参考。 
    //用于标记装甲板、能量机关的角点，框出候选的目标
    ~~~

  - 颜色空间转换

    ```c++
    cvtColor()		//将图片从一个颜色空间转换到另一个颜色空间
    split()		//把图片的不同通道进行拆分，放入不同的Mat
    subtract()		//将两张矩阵的每个元素相减
    //这在自瞄中将用于RGB到GRAY和HSV等空间的转换和颜色通道的分离。
    ```

    - 阈值

    ```c++
    threshold()		//阈值操作，对一个特定的分量与阈值进行比较，大于阈值则全部设为某个值，小于阈值设为另一个值
    inRange()			//进阶版本，可以确定一个分量是否在一个区间内
    //我们使用这两个函数来筛选特征，对拆分后的颜色空间进行操作以屏蔽不感兴趣的部分
    ```

    - 滤波与平滑

     ```c++
    blur()				 	//加权模糊图像
    GaussianBlur()		 	//高斯加权模糊
    medianBlur()		    //中值滤波
    bilateralFilter()	 	//双边滤波
    //用于对图像进行降噪处理，或是抹去小光斑等
     ```

    - 形态学操作

    ```c++
     erode()				   //腐蚀操作，二值图的边缘或收缩
     dilate()			 	   //膨胀操作，二值图的边缘会扩张
     getStructuringElement()   //获得结构元素（核）
     morphologyEx()     	   //更多的形态学操作，包括Opening,Closing,Morphological Gradient,Top Hat,Black Hat等
    //用于增强图像的某些特征
    ```

    - 其他图像算子

    ```c++
     Sobel() 	  //微分运算，检测边缘，微分会使得图像中像素强度（某个分量）变化最大的部分为极值
     Laplacian()  //二阶微分，检测边缘，二阶微分会使得图像中像素强度变化最剧烈的部分为零
    //寻找图像中的边缘
    ```

    **滤波、平滑、形态学操作等都属于使用图像算子对图片进行卷积操作**

    - 寻找/画出轮廓 + 矩形/椭圆拟合

    ```c++
     floodFill()    //漫水法，常用于寻找轮廓的预处理操作，和“画图”软件中“油漆桶”工具有相同的效果
     findContour()	//寻找二值图中的轮廓，并保存为一组点，算法类似于漫水法，遍历所有像素并查找相邻像素
     drawContour()  //根据一组点画出轮廓
    //承接上面的各种预处理，用于找出图像中的轮廓并进行下一步操作
       

   minAreaRect()	 //通过轮廓点，拟合出最小面积的RotatedRect
   boundingRect()	 //通过轮廓点，找到其外接矩形Rect(水平)
   fitEcllipse()	 //通过轮廓点，用最小二乘法拟合出一个外接椭圆，函数会返回椭圆的内接旋转矩形 RotatedRect
   minEnclosingCircle() 	//通过轮廓点，找到最小面积的包含圆（注意不是外接圆）
  //将轮廓点转换为更容易处理的形状对象

    ```c++
   - 仿射、投影变换
   remap()					//根据给定的映射（函数）改变图像中每个像素点的位置        
   warpPerspective()			//进行透视变换  
   getPerspectiveTransform() 	//获得透视变换所需的矩阵（4个点）  
   warpAffine()				//进行仿射变换  
   getAffineTransform() 		//获得仿射变换所需的矩阵（3个点，为什么比透视少一个点？）
   //一般用于把图像根据变换关系转化成正视图以便进行模板匹配、SVM匹配等操作
    ```

- **imgcodecs模块**

  ~~~c++
  imread()		//根据路径读取一张图片
  imwrite()   	//向对应路径写入一张图像
  ~~~

- **highgui模块**

  ``` c++
  imshow()/*在指定名称的窗口中显示一张图片，注意和waitKey()配合使用否则可能导致异常，用于查看一些算法处理后的结果，waitKey()的参数为图片显示的时间*/   
      
  //以下这个组合可以极大地方便参数调试，在程序运行的过程中通过回调函数，可以实时修改参数值
  //这种根据用户或客户端的请求进行动作的编程范式被称为“响应式编程”，在GUI、OS和应答服务中很常用
  nameWindow() 		//新建一个空窗口  
  createTrackBar()	//创建一个拖条，传入相关的参数可以实现参数调节  
  getTrackBarPos()	//返回拖条所在的位置     
      
  //这个组合能够通过键盘和鼠标向程序传递参数，改变程序的状态，调试的时候非常好用  
  setMouseCallback() //设置鼠标的回调函数  
  waitKey() 	   //从键盘读取输入
  ```

- **其他模块**

  ~~~c++
  imread()		//根据路径读取一张图片
  imwrite()   	//向对应路径写入一张图像
  ~~~

  dnn模块；deep neural network模块，随着深度网络的流行OpenCV自然也提供了对包括onnx、tensorflow、pytorhc、caffe等模型推理的支持（不支持训练）。基本的api封装的非常简洁，以分类网络为例，分别运行`readnet()`，`blobFromImage()`，    `setInput()`，`forward()`就可以得到结果。对于检测网络和分割网络，还要进行后处理，不过`cv::DNN`也提供包括`NMS()`等简单的后处理。如果要在程序中嵌入一些轻量的网络，使用DNN模块足堪使用。若有复杂的pipeline和前后处理，建议还使用特定的推理框架（OpenVINO、TensorRT、NCNN、MNN等）

- - - *<u>学完上述内容你就基本具备了完成传统装甲板识别算法的能力</u>*

#### 3.4.2 目标检测 (神经网络识别)

- 谈到这个，首先应该学习的是python，因为pytorch框架是基于python语言，在有着c和c++的基础上，学习python很简单，掌握基本语法和使用即可。

- pytorch

  它是一个功能强大且易于使用的深度学习框架，提供了丰富的工具和库，用于简化深度学习模型的开发。

  **动态计算图**：PyTorch使用动态计算图，这意味着在编写代码时可以即时定义、修改和执行计算图。这使得模型的构建更加灵活，并且可以方便地进行调试和可视化。

  **易于使用**：PyTorch具有直观的API设计，使得用户可以快速上手并迅速实现他们的想法。它提供了丰富的预定义模型和模块，方便用户构建神经网络。

  **动态权重更新**：PyTorch允许在训练过程中实时更新模型的权重，这对于处理递归神经网络等动态结构非常有用。

  **支持GPU加速**：PyTorch能够直接利用GPU进行计算，通过使用CUDA和CuDNN等库，用户可以在GPU上高效地进行深度学习模型的训练和推断。

  **丰富的计算库**：PyTorch提供了广泛的计算库，包括张量操作、数学函数、卷积运算、图像处理等，方便用户进行各种计算任务。

  **灵活的模型部署**：PyTorch支持将训练好的模型导出为各种格式，如ONNX（开放神经网络交换）和TorchScript，以便在不同的平台上进行部署和推理。

​       安装方式如下；

~~~c++
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
//上述为conda环境的安装方式距离，具体安装根据版本在官网上寻找
~~~

 学习完pytorch可以写一个对其自带数据集MNIST写一个神经网络训练代码，实现对手写数字的模型训练和识别。

接下来会接触各种模型onnx，pt，wts，xml，engine等

- yolov5

  是一种基于PyTorch的目标检测算法，是由Glenn Jocher在2020年发布的。YOLO（You Only Look Once）是一系列快速的目标检测算法，而YOLOv5是YOLO系列中的最新版本之一。与YOLOv4相比，YOLOv5具有更高的性能和更好的精度，并且在实时视频处理和嵌入式系统上表现更出色。

  可以用这个训练一版自己的多分类模型

  以下是YOLOv5的一些特点和亮点：

  - **基于PyTorch**：YOLOv5是使用PyTorch框架实现的，这使得它易于理解、修改和扩展，并且能够充分利用PyTorch的丰富生态系统。
  - **轻量级**：YOLOv5相对于以往的版本更加轻量级，同时在保持高精度的情况下减少了模型的大小和计算复杂度。
  - **多种变体**：YOLOv5提供了多种变体，包括不同的模型大小（如S、M、L、X等）和不同的骨干网络（如CSPDarknet、EfficientNet等），以满足不同场景和设备的需求。
  - **优化的训练流程**：YOLOv5通过改进的数据增强、学习率调整和模型蒸馏等技术，优化了训练流程，提高了模型的收敛速度和精度。
  - **易用性**：YOLOv5提供了简单易用的接口，用户可以轻松地进行训练、推理和部署。
  - **多种应用**：YOLOv5在目标检测领域被广泛应用，包括人体检测、车辆检测、工业检测等各种场景。

- yolox-nano

  模型相较于yolov5那种大模型更小，运算平台运算更快，帧率更高。当然模型越小，对应精度也会损失，但经过实践，这款模型满足精度需求，同时运算更快。当然算法当中使用ROI也有此作用，毕竟处理的图像小了。

- ==四点模型==

  已经了解过距离解算或pnp解算的前提下会发现，yolo系列的bbox始终是一个标准的矩形，不会随着装甲板的倾斜而贴合灯条的四个角点，这可要怎么解算呢？别担心，我们还有特征点检测。前面介绍的网络在检测头部会回归bbox的四个点，那么我们通过修改检测头的结构和用于训练的数据集，让网络直接**回归装甲板灯条的四个角点**不就行了吗！这正是上交2021赛季开源的[解算方案](https://github.com/Harry-hhj/CVRM2021-sjtu)，他们修改了YOLOv5的检测头，并以最小化回归得到的角点和数据集中角点之间距离的**MSE**（mean square error）为优化目标对lossfunction进行修改，便得到了能检测四个角点的网络。沈航2022的[赛季开源](https://github.com/tup-robomaster/TUP-InfantryVision-2022)也使用了四点检测网络，而且他们还提供了基于YOLOX-nano修改的[训练代码](https://github.com/RangerOnMars/TUP-NN-Train)。当然，使用这种方法需要自己制作数据集哦（RM视觉数据站上也有大量标注好的四点数据集）。一些现成的关键点检测网络也可以用于我们的检测任务，只要把关键点个数调整为4个，并把自己的数据集改成对应形式即可。
  
- 神经网络仓库的使用（以沈航为例）

  - 数据集的制作

    - 可使用上交标注工具（装甲板标注）；标注出来的数据集格式为yolo四点

    - 或者实用性更广泛的labelme（装甲板和能量机关都适用）；标注出来的数据集格式为json

    - 数据集的格式有yolo，yoloface，json，yolo四点，json四点

    - 标注出来格式相互转换可以使用脚本实现，一个带有python包和指定包的conda环境解决

    - 注意训练时对各类别的数据集的分配要合理，同时还有正负样本的要求
    
    - 制作时可以做一些数据增强等
    
  - 训练时的操作
  
    由于使用的已经调整网络结构，超参数等的仓库，所以只要将关键点个数，训练epoch和模型输入尺寸等即可。
  
    训练过程中可借用tensorboard工具查看损失曲线，防止过拟合。
  
    炼出的模型可以使用Netron工具查看模型情况。

- 模型的使用

  模型虽小但在机器人上的小电脑没有神经网络加速工具的支持，同时其也提供了模型推理的框架。

  目前使用有tensorrt（nx上使用，gpu独显加速），openvino（nuc上使用，cpu或集显加速）两种，

  当今神经网络领域，自动驾驶等方向使用的便是tensorrt框架，因此对其首要学习。但是参考沈航四点模型等开源实践中发现，帧率最高的模型推理代码在nuc中，因此openvino的学习和使用也是必要的。

  指定模型路径然后进行推理从而获取回归的点位，框选机器人装甲板的位置。

  <u>*到此为止，使用神经网络模型推理进行识别便能实现了*</u>，不得不说，神经网络的识别十分稳定，而且不需要像传统自瞄日常的苦苦调参，真可谓身心舒畅。

  识别做完了，接下来该筛选跟踪，预测，补偿等姿态解算的工作了 :smiley:

  

#### 3.4.3 姿态解算

##### 筛选跟踪

所谓做筛选，是因为相机读入的画面中肯定不可能是一个装甲板，但是你此时要击打的装甲板是唯一的，因此要选择一个最优的装甲板来拿出来做姿态解算，通常我们先选择离画面中心点最近的一个，丢失以后选择离上一帧最近的装甲板，如果有特定兵种的出现要优先击打（如英雄），也可以加进框架中。

在筛选出来以后，理应一直跟踪此块装甲板，要不然的话一直切换装甲板，整个云台一直乱晃

计算tracker的切换打击分数,由装甲板旋转角度,距离,面积大小决定

英雄约束与上次目标约束，若检测到危险距离内的英雄直接退出循环，若检测到存在上次击打目标,时间较短,且该目标运动较小,则将其选为候选目标,若遍历结束未发现危险距离内的英雄则将其ID选为目标ID

有对小陀螺的检测，分为小陀螺下的筛选击打，常规击打。

##### 预测

将pnp解算出的平移向量,即相机坐标系下的坐标，通过四元数的旋转矩阵转换成陀螺仪坐标系下的坐标（也可视为世界坐标）进行预测

###### - 粒子滤波（particle filter）

PF可以看作是UKF的进化版。UKF要求用服从高斯分布的一组点经过转换之后去通过采样来得到**新的高斯分布**进而近似真实的状态分布；而PF则是不再追求用高斯分布去近似真实分布，直接用一组点经过模型转换后再采样，用此后验数据来**近似任意分布**。它们的区别在于UKF是通过一组假设在通过系统的转换后仍然服从高斯分布的采样点来求其**参数μ和σ**，是一种参数估计方法；PF是用已知的采样点数据去**求未知的任意分布**，或者从另一个角度来说，就是干脆不给出后验分布函数（不用分布来描述，就是一堆离散化的点），直接根据粒子的权重进行融合得到**状态的一种可能假设**。根据大数定律，想要通过采样近似一个分布，样本越多则结果越接近真实分布，因此PF在“撒豆子”时会使用比UKF多得多的样本点，所以它本质上是一种非参数化的蒙特卡洛方法。

而在代码中对位置和速度进行了预测（代码主要参考沈航开源部分）

主要调整的是粒子的数量，理论上来说粒子数越大效果越好，粒子数量为无穷时即为真实分布

过程噪声,可以增加粒子多样性,提高滤波器鲁棒性,增大后可以提高滤波器响应速度,但也会造成坐标抖动，过程噪声矩阵，会在重采样时为粒子叠上方差为该矩阵对角线元素的高斯分布，以求增加粒子多样性

观测噪声, 影响重要性密度函数,会改变粒子权重,增大可以平滑预测值,但也会增大预测延迟，观测噪声矩阵，对角线元素即为重要性采样函数（高斯分布右半侧）的协方差。一般来说，不超过过程噪声的两倍

###### - 卡尔曼滤波(kalman filter)

ExtendedKalman filter（主要是对华中科技的开源进行了学习和使用）

最后预测的点是滤波后的xyz加上xyz的微分值乘以deltat。

对于卡尔曼滤波，我们要先明白各个参数的含义，为状态估计量，也即你需要通过卡尔曼滤波得到的参数，在我们所使用的卡尔曼滤波中，位置和速度是我们需要的，由于需要在三维空间中进行状态估计，故有六维。

测量值说就是传感器pnp测量得到的值，对三维点坐标作非线性变换，最后用yaw,pitch,distance作为测量值。（华中科技学习的是上海交大的球面坐标系开源）

我们需要调节的卡尔曼滤波矩阵理论上有三个；

P矩阵是状态估计协方差，其初值会影响迭代的收敛速度，但一般设定为单位阵然后自动收敛即可。

R 矩阵在该模型中为测量噪声矩阵，大小为3x3,一般只需要设置其对角元素，即认为测量量之前无相关性。测量量分别为yaw,pitch,distance,根据R矩阵的定义,对角元素的值应当为测量量的方差，目前的做法是测出不同距离的distance方差，然后用一条直线拟合出来，每次都不断计算更新distance的方差。比设定一个固定的值要更稳定一些。同时设置了一个倍数设置参数。

Q矩阵是一个6x6的矩阵，那么应当有六个值需要调，但是仅仅设置Q矩阵的对角阵的话，有一个问题就是默认了一个轴的速度与那个轴的位置值无关，这显然是不成立的。那么速度值和位置值的过程噪声必然存在一些关联。可以通过"CV运动模型"来了解相关内容。在得到同一个轴之间的关联之后，需要调节的参数就只剩下三个了，分别是x,y,z轴的过程噪声。其实发现x,y轴应该是等效的(因为存在陀螺仪绕z轴的旋转)，那么x,y轴的过程噪声也应当设置为一致的，在将需要统计计算得到的结果统统得到以后，便可以开始进行调参，通常需要调节的只有两个过程噪声参数而已。

如果这两种对比图像都画在同一幅图（imshow）上，或者也可以实时获得原始和预测数据然后生成曲线，可视化调参将会很方便。同时调节时注意噪声小，收敛快，更相信测量和状态转移方程，对应的噪声大，收敛慢。

##### 补偿

子弹在飞行过程中会受到重力，空气阻力等多个力的作用。在这些力的作用下，子弹的弹道呈类似抛物线，一般来说，我们进行弹道补偿，目前使用的是迭代法求解弹道学微分方程的方法来解算弹道所需的补偿值。

迭代法是一种常用的数值方法，可以用于求解弹道学微分方程以及计算所需的补偿值。下面是使用迭代法求解弹道学微分方程的基本步骤：

1. **建立弹道学微分方程：** 根据具体的弹道学问题，建立描述弹道运动的微分方程。涉及到质量、速度、加速度、阻力等变量。
2. **离散化解算区间：** 将求解区间离散化为若干个小时间步长，确定每个时间步长的大小。
3. **设定初始条件：** 给定初始条件，如弹道起始位置、速度、质量等。
4. **迭代求解：** 从初始条件开始，通过迭代计算逐步逼近解析解。具体步骤如下：
   - a. 在每个时间步长上，计算当前时刻的补偿值。
   - b. 使用当前时刻的补偿值和已知的初始条件，进行微分方程求解，得到下一个时间步长的状态。
   - c. 根据所得到的下一个时间步长的状态，更新补偿值。
   - d. 重复步骤 b 和 c，直到达到所需的时间或满足一定的终止条件。
5. **输出结果：** 当迭代过程结束后，得到所需的补偿值。

需要注意的是，使用迭代法求解弹道学微分方程是一种数值近似方法，结果的精度取决于时间步长的选择和终止条件的设置。较小的时间步长和更严格的终止条件可能会提高结果的精度，但也会增加计算的时间和复杂度。

##### 击打逻辑

可以考虑将枪口对准对方机器人的中心，当任意一块装甲板那的预测极大位置包含中心时开火；或者跟随预测一起运动，保证预测量不超出机身范围，一旦超出就切换瞄准下一块装甲板。对于第二种方法也未必需要计算底盘旋转的角速度，直接用经验法/数据拟合法建立装甲板速度和出现在正面的时间的**相关关系**也是一种不错的方法。

若没有建立底盘模型，只是为每个能看到的装甲板维护跟踪器，可以保存最后一帧装甲板消失的位置和新装甲板出现的位置，**取两者的中点作为打击点**，采用和前者相同的策略（也可以不自动开火，就对准中央让操作手决定开火时机，减少云台晃动以提升操作体验）；或始终瞄准在**旋转方向上落后**的（也就是第二块）装甲板，因为它的“存活时间”比较长，预测的有效时间相应的会更长。一旦新的装甲板出现，立刻转向瞄准它，此时还可以将老装甲板的信息继承给新出现的装甲板以加速收敛，不过此法只对静止的陀螺有较好的效果，否则因为**速度耦合**，击打运动陀螺的继承关系很难确定，且有速度误差，参考这些方法可以做相关的的开火逻辑和自动击打策略。



### 3.5 队里面使用视觉方案

#### 3.5.1 rm_vision

该项目是华南师范大学陈君开源的一台贴合RM赛场的[自瞄项目](https://flowus.cn/lihanchen/share/0d472992-f136-4e0e-856f-89328e99c684),针对该项目，北极熊战队对此有详细教程解释（很详细）。

##### 使用方法

1.君瞄使用的识别方案为传统视觉，因此调试识别阈值参数是很重要的一环，也是因为这套自瞄算法的整车观测器对于识别的pnp精度要求很高，对于传统的svm数字识别君瞄将其换成一个简单全连接层的神经网络模型从而一定程度提高识别的鲁棒性，（该模型针对二值化后的数字roi进行训练的）

2.调试过程中有专门的yaml进行参数整定，包括foxglove调试器，当然ros2自带的rviz+rqt也可以。

3.上位机将整车状态发到下位机，然后下位机做弹道补偿，线性预测，开火逻辑。

#### 3.5.2 tup-robomaster

其中包含了[沈航开源的神经网络项目](https://github.com/tup-robomaster),他们的视觉方案采用的清一色的神经网络，开源项目也很丰富，目前队里使用的神经网络方案大多参考他们，神经网络训练仓库，模型推理代码，雷达站都有涉及，但随着近年来各家的神经网络开源逐渐丰富，沈航的项目并不是唯一的，有很多各领域更好的开源方案更加可供参考，详情见开源汇总。

### 3.6 一些心得和建议

#### - 代码调试

- 自启动脚本的使用，机器人通电到小电脑后，我们总不能插上显示屏然后运行程序吧，因此开机自启动显得很是必要。\#!/bin/bash写在开头代表使用shell解释器自动运行，在系统中设置这个sh文件的路径，然后完事
- 安装包的时候可以这样借助国内源增加速度    命令行示例：pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
- 模型转换的时候一定要注意环境的版本
- 平时遇到问题解决以后最后进行记录，肯定会再遇到，这样可以提高工作效率
- 写代码的时候遇到问题多动手尝试，写完之后一直寻找错误不如直接运行看报错，更精准（手动狗头）。在学习神经网络推理的时候，阅读开源是必不可少的，但是开源良莠不齐，如何判断？先运行，推理能跑，好了，没问题，开学，不然学到最后都运行不起来就难崩了。当然，多从优秀的开源学习是一定的，可能自己写的确实不如大佬们的，但是总要试一试自己思路的体现，对开源取其精华，去其糟粕。
- 之后心得继续补充

#### - 遇到问题怎么办

学习一定要循序渐进，如果某些知识要求一些前置知识，请务必把基础夯实再开始学习前者，否则很容易不知其所云。另外，一定要**成体系地进行学习**，尽量一次性建立该学科或该领域的主干，不要总是四处寻找碎片化的知识。当你遇到一个难以解决的bug或者异常，**请先问问自己**，我是否有相关领域的基础知识，如果有了再尝试查找资料解决；**如果没有**，请立刻着手学习，否则你可能根本无法解决这个问题，或是即使查到了也不理解它到底是如何帮助你修复这个bug的，只是按部就班地根据其解决步骤**机械地复制**罢了。对于一些一直无法理解的公式推导或者知识，另外，在上述遇到问题的情况下，你自己可能无法解决（**但是也请仔细看一看报错信息，这并不困难，而且很多时候其实很快就能定位问题所在**，不要看到一片红色的error就不知所措），这时候就要利用搜索引擎或者向它人求助。一般来说，技术类的问题，搜索的时候，请使用陈述句而不要加入语气词/问号/疑问词等。如“C++ vector的初始化”就比“怎么样初始化一个vector”好得多。善用搜索引擎的**规则搜索**，在该软件平台的官方网站/说明文档中寻找解答是一个很好的方法。对于编程问题，CSDN可以搜索，谨慎使用CSDN上的解决方法，很多完全不奏效甚至会造成意想不到的后果，并且质量良莠不齐，存在大规模的复制黏贴。

 如果搜索引擎解决不了的，可以尝试去使用**AI**来解决，在有些情况下导致的错误都是由于很小很小的一个点，甚至有时候有可能只是变量名输错了，但是编译器又发现不了，这时可以借助**AI**来寻找错误的来源。一般到这里能解决大部分的错误。如果在使用一些国内资源比较少的一些项目时，不妨去外网查一下，看一下别人GitHub项目中的issue，Wiki，还有官方的文档，很多问题能迎刃而解。做一个程序员一定一定要学会看文档，**请仔细阅读每一个你使用到的开源项目的文档**,不仅仅局限于readme，对于深度学习中的如tensorrt，openvino这样的官方文档就丰富到就像教学资料。 如果以上步骤都无法解决，可以尝试询问一下学长或者队友。**前提是你上面三个步骤都做完了，但是没有得到有效的解决方案，不然别人也不一定想理你**qwq。这里的学长并不限制于本校的学长，可以向其他学校的学长一起交流问题。但是，一定要**注意提问的方式**，不然很有可能会没人理你qwq。RM视觉救命群就是不错的推荐。



## 4. 致谢

在我学习的道路上，遇到了很多不会的知识和棘手的问题，在这里要特别感谢一下曾经帮助过我的一些人，:kissing_smiling_eyes:

2024继续加油！！
