### 一、网络摘要

- **FNN:全连接神经网络**（Feedforward Neural Network）

- **ResNet**: ResNet（残差网络）是一个深度卷积网络，特别之处在于其使用了残差连接（skip connections）来避免梯度消失问题，通常应用于更深的网络结构。



### 二、代码解读

### 1.FNN.py 

### **NeuralNet 模型**

这是一个简单的全连接神经网络，它包含两个全连接层和一个ReLU激活函数。这个模型是传统的前馈神经网络，并不包括卷积层或任何形式的跳跃连接，因此与ResNet18相比，它的特征提取能力较弱。

#### 网络层分析：

- `fc1`: 第一个全连接层，将28x28的输入展平（784个输入节点）并映射到500个隐藏单元。
- `relu`: 激活函数，引入非线性变换。
- `fc2`: 第二个全连接层，将隐藏层的输出映射到10个输出类别。

这个模型的结构简单，但它的训练和推理效率通常不如卷积神经网络（CNN），尤其是在图像分类任务中，因为它没有卷积层来提取空间特征。

#### 优化器和损失函数：

- **SGD优化器**（随机梯度下降）用于训练模型。相比Adam，它的收敛速度可能较慢，但也更容易控制。
- 使用了 **交叉熵损失函数**（`CrossEntropyLoss`），与ResNet18模型相同，适用于多分类任务。

#### 超参数

1. **`learning_rate `**：学习率，控制每次更新模型权重时步长的大小。较小的学习率可能导致训练收敛较慢，而较大的学习率可能导致训练过程不稳定。
2. **`batch_size `**：批次大小，表示每次迭代中，模型用于训练的样本数量。批次大小的选择会影响训练速度和模型的泛化能力。较小的批次大小（如32或64）通常会带来更好的泛化能力，而较大的批次大小可能会加快训练过程。
3. **`num_epochs `**：迭代次数，表示整个数据集将被用来训练模型的次数。

### 2. **ResNe.py**

### **ResNet18 模型**

其定义的基于ResNet架构的网络，ResNet是一种带有残差连接的深度卷积神经网络。这种结构的核心思想是“跳跃连接”，即直接将前面某些层的输出加到后面层的输入中，它通过残差块（Residual Block）解决了深层网络训练中的梯度消失问题。

#### 网络层分析：

 **`ResidualBlock`** 是实现残差学习的关键模块，主要的操作包括两个卷积层，并且使用短路连接（skip connection）来进行残差学习

- **卷积层1 (`conv1`)**：使用 3x3 的卷积核进行卷积，输入通道数为 `inchannel`，输出通道数为 `outchannel`，步长 `stride=1`，填充 `padding=1`。

- **Batch Normalization (`bn1`)**：在卷积层之后进行批量归一化，以加速训练并保持梯度稳定。

- **卷积层2 (`conv2`)**：第二个 3x3 的卷积层，输入输出通道数为 `outchannel`，步长和填充同样为 1。

- **Batch Normalization (`bn2`)**：第二次批量归一化。

- **残差连接 (`extra`)**：如果输入和输出的通道数不同（例如通过卷积层后改变通道数），使用一个 1x1 的卷积层将输入映射到输出通道数，保证输入与输出可以相加，形成残差连接。

- **ReLU 激活函数**：最后应用 ReLU 激活函数增加非线性，使得网络能够学习复杂的模式。

  

- `conv1`: 第一层卷积层，用于从输入图像提取特征。输入通道数为1（灰度图），输出通道数为32，卷积核大小为3，步幅为3，且没有填充。

- `layer1, layer2, layer3, layer4`: 这四个 `ResidualBlock` 组成了ResNet的核心部分，每个 `ResidualBlock` 都包含了卷积层、批量归一化和跳跃连接。这些块可以增强特征学习的能力，网络可以学习到更深层次的特征。

- `outlayer`: 最后使用一个全连接层，将特征映射到10个类别的输出。这里假设输入图像是 `28x28` 大小的，因此通过适应性池化（`adaptive_avg_pool2d`）将特征图大小缩减为 `1x1`，然后将其展平。

#### 优化器和损失函数：

- 使用了 **SGD优化器**，这是一种自适应学习率优化方法，通常表现较好。
- **交叉熵损失函数**（`CrossEntropyLoss`）用于多分类任务，适合处理分类问题。

- **ResNet18** 模型通过深层的残差连接解决了传统深度神经网络在训练中遇到的梯度消失问题，允许更深的网络进行有效训练。
- 每个 **ResidualBlock** 包含两个卷积层和一个残差连接，用来保证信息流畅传递。
- 模型采用了 **全局平均池化** 和 **全连接层**，适合用于图像分类等任务。

#### 训练部分

训练前设置了**损失函数和优化器**

进行**数据处理**：

- `transforms.ToTensor()` 将图像转换为 Tensor，并且自动将像素值从 `[0, 255]` 缩放到 `[0, 1]`。

- `transforms.Normalize(mean=[0.1307], std=[0.3081])` 用于对 MNIST 数据进行标准化处理，使图像的像素值符合零均值和单位标准差，有助于加速模型训练。

将处理完的数据进行**数据集和数据加载器**

- 使用 `datasets.MNIST` 下载 MNIST 数据集，并应用上述的 `transform` 进行图像预处理。

- `DataLoader` 用于批量加载数据并打乱顺序（`shuffle=True`）。这对于训练过程中每一轮都能获得不同的样本顺序是非常重要的。

训练详解

- 基本的训练循环，每次遍历一个 batch，执行前向传播、计算损失、反向传播、梯度裁剪、参数更新

- 同时，使用学习率调度器 `StepLR` 来逐步降低学习率，以提高训练的稳定性和最终性能。

- 输出基本日志，帮助开发者监控训练过程。

- **设置训练模式**：`model.train()`，切换到训练模式。

- **学习率调度**：`StepLR` 每 10 个 epoch 降低学习率。

  尽管设置了 `SGD` 优化器，但使用学习率调度器会有一定优化，尤其是当训练过程中模型可能出现停滞时。通过调度器，可以逐渐降低学习率，帮助模型跳出局部最优，找到更好的解。

- **数据加载**：从 `train_loader` 获取每个 batch 的数据。

- **数据转移**：将数据和标签移动到指定的设备（CPU 或 GPU）。

- **清除梯度**：`optimizer.zero_grad()`，防止梯度累积。

- **前向传播**：`output = model(data)`，计算模型的预测值。

- **计算损失**：`loss = criterion(output, target)`，计算预测与真实值之间的损失。

- **反向传播**：`loss.backward()`，计算梯度。

- **梯度裁剪**：`clip_grad_norm_` 防止梯度爆炸。

- **更新参数**：`optimizer.step()`，根据梯度更新模型参数。

- **打印进度**

- **更新学习率**



## 三、设计总结

### **1. 网络架构设计**

这一步是神经网络模型的核心，包括选择网络的结构和层类型，常见的模块有：

#### **输入层**

- 网络的输入层接收数据。它的维度应该与输入数据的形状匹配。例如，如果处理图像，每个图像可能是一个28×28的像素矩阵，那么输入层的维度就是28×28。

#### **隐藏层**

- **卷积层（Convolutional Layer）**：用于提取图像中的局部特征，常用于计算机视觉任务。通过卷积核对输入进行卷积操作。
- **全连接层（Fully Connected Layer, FC）**：每个神经元都与前一层的每个神经元连接，常用于最终的分类或回归任务。
- **池化层（Pooling Layer）**：用于降低特征图的空间维度（如最大池化、平均池化），减少计算量和避免过拟合。
- **激活层（Activation Layer）**：通常用于每层的输出，使用非线性函数（如ReLU、sigmoid、tanh）增加网络的表达能力。
- **Dropout层**：用于正则化，通过随机丢弃一定比例的神经元来防止过拟合。

tips：所有模型都要继承自Model

最少实现两个成员方法

构造函数 初始化：__init__()

前馈：forward()

#### **输出层**

- 输出层的维度通常与任务的类别数或预测目标相匹配。例如，在二分类问题中，输出层可能有一个神经元，使用sigmoid激活函数输出0到1之间的概率；在多分类问题中，输出层的神经元数等于类别数，通常使用softmax激活函数。

### **2. 选择损失函数**

损失函数用于衡量模型输出与实际标签之间的差异，常见的损失函数有：

- **均方误差（MSE）**：常用于回归问题，计算预测值与真实值之间的均方差。
- **交叉熵损失（Cross-Entropy Loss）**：常用于分类问题，衡量预测类别与真实类别的差距。
- **Hinge损失**：用于支持向量机（SVM）等分类任务。

### **3. 选择优化器**

优化器用于根据损失函数的反馈，调整神经网络的参数。常见的优化器有：

- **梯度下降法（SGD）**：基础的优化方法，使用梯度下降更新权重。
- **Adam**：结合了Momentum和自适应学习率的优化方法，通常在大多数深度学习任务中效果很好。
- **RMSprop**：类似于Adam，使用自适应学习率的方法来加速收敛。

### **4. 训练模型**

- **前向传播**：将输入数据传递通过网络的每一层，得到输出。
- **计算损失**：使用损失函数计算输出结果与真实标签之间的误差。
- **反向传播**：通过反向传播算法计算每个参数（权重和偏置）的梯度。
- **参数更新**：使用优化器（如Adam、SGD等）根据计算出的梯度来更新网络的参数。
- **迭代训练**：通过多次迭代（epochs）不断优化模型，直到损失函数收敛。

### **5. 模型评估与验证**

- **验证集**：在训练过程中，使用验证集来监控模型的性能，避免过拟合。
- **测试集**：训练完成后，使用测试集来评估模型的最终性能。
- **评估指标**：根据问题类型选择合适的评估指标，例如准确率（Accuracy）、精确率（Precision）、召回率（Recall）、F1分数等。



## 四、比较总结

- **ResNet18** 是一个复杂的卷积神经网络，具有残差连接，适用于图像分类任务，尤其是当数据较为复杂时。它能够自动从图像中提取层次化的特征，因此能够更好地处理图像数据。

- **NeuralNet** 是一个简单的全连接神经网络，它适用于较简单的任务或者较小的数据集。由于没有卷积层，它不能有效处理图像数据的空间结构。

- 如果只是想做一些简单的实验，或者数据量非常小，可以使用 **NeuralNet**，但对于更复杂的任务，ResNet18会显得更为强大。

- 如果正在处理图像数据，尤其是较复杂的图像数据（如彩色图像、自然场景等），建议使用 **ResNet18**，它会提供更好的性能。

  

## 五、Demo

使用了预训练的 ResNet18 模型。

- **选择设备：**
  - 会根据当前系统是否有可用的 GPU，选择将模型加载到 GPU (`cuda`) 或者 CPU (`cpu`) 上。
- **加载模型权重：**
  - 加载预训练模型权重文件，并将其加载到上一步选择的设备（GPU 或 CPU）中。
- **切换模型为评估模式：**
  - `model.eval()` 设置模型为评估模式，关闭如 Dropout 等训练时特有的操作，使其可以用于推理和预测。

它包括两个主要功能：

- **静态图像处理：**
  - 通过 `process_and_predict_image` 函数，加载输入的图像文件，进行预处理（灰度转换、尺寸调整、标准化），然后将图像传入 ResNet18 模型进行预测，并在图像上标出预测结果。
- **实时视频流处理：**
  - 通过 `process_and_predict_video` 函数，从摄像头捕获实时视频流，逐帧进行图像预处理并输入到模型进行预测，实时显示预测结果。如果按下 'q' 键，则退出视频流处理。

整体流程包括图像的加载、预处理、模型预测、结果显示和相关错误处理。
